{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Step 1: Download stock data from Yahoo Finance\n",
        "def download_stock_data(ticker, period='5y'):\n",
        "    stock_data = yf.download(ticker, period=period)\n",
        "    return stock_data\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "def preprocess_data(data, feature_col='Close', seq_length=60):\n",
        "    # Use 'Close' prices to predict trends\n",
        "    data = data[[feature_col]]\n",
        "\n",
        "    # Normalize the data using MinMaxScaler\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Create sequences of data points for LSTM input\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(scaled_data)):\n",
        "        X.append(scaled_data[i-seq_length:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape the data to be compatible with LSTM (samples, timesteps, features)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    return X, y, scaler\n",
        "\n",
        "# Step 3: Build the LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(units=1))  # Predicting a single output value (next price)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Step 4: Train the LSTM model\n",
        "def train_lstm_model(model, X_train, y_train, epochs=50, batch_size=64):\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "    return model\n",
        "\n",
        "# Step 5: Make predictions and evaluate\n",
        "def predict_and_evaluate(model, X_test, y_test, stock_scaler):\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform only the stock price column\n",
        "    predictions_stock_price = stock_scaler.inverse_transform(predictions)\n",
        "    y_test_stock_price = stock_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Calculate the Root Mean Squared Error (RMSE) on the stock prices\n",
        "    rmse = np.sqrt(np.mean((predictions_stock_price - y_test_stock_price) ** 2))\n",
        "    return predictions_stock_price, rmse\n",
        "\n",
        "# Step 6: Trading simulation logic\n",
        "def simulate_trading(predictions, actual_prices, dates, initial_balance=10000, shares=0):\n",
        "    balance = initial_balance\n",
        "    total_shares = shares\n",
        "    trade_log = []\n",
        "\n",
        "    for i in range(1, len(predictions)):\n",
        "        predicted_price = predictions[i]\n",
        "        actual_price = actual_prices[i]\n",
        "        date = dates[i]  # Make sure 'date' is a datetime object\n",
        "\n",
        "        # Ensure you're not comparing dates with prices\n",
        "        if predicted_price > actual_prices[i-1] and balance > actual_price:\n",
        "            shares_to_buy = balance // actual_price\n",
        "            balance -= shares_to_buy * actual_price\n",
        "            total_shares += shares_to_buy\n",
        "            trade_log.append(f\"Bought {shares_to_buy} shares at {actual_price} on {date}, Balance: {balance}, Shares: {total_shares}\")\n",
        "\n",
        "        elif predicted_price < actual_prices[i-1] and total_shares > 0:\n",
        "            balance += total_shares * actual_price\n",
        "            trade_log.append(f\"Sold {total_shares} shares at {actual_price} on {date}, Balance: {balance}\")\n",
        "            total_shares = 0\n",
        "\n",
        "    # Final balance after selling any remaining shares\n",
        "    if total_shares > 0:\n",
        "        balance += total_shares * actual_prices[-1]\n",
        "        trade_log.append(f\"Final Sale of {total_shares} shares at {actual_prices[-1]} on {dates[-1]}, Final Balance: {balance}\")\n",
        "\n",
        "    profit_loss = balance - initial_balance\n",
        "    return trade_log, profit_loss\n"
      ],
      "metadata": {
        "id": "kBSjrkNl9wxK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Trading simulation logic\n",
        "def simulate_trading(predictions, actual_prices, dates, initial_balance=10000, shares=0):\n",
        "    balance = initial_balance\n",
        "    total_shares = shares\n",
        "    trade_log = []\n",
        "\n",
        "    for i in range(1, len(predictions)):\n",
        "        predicted_price = predictions[i]\n",
        "        actual_price = actual_prices[i]\n",
        "        date = dates[i]  # Make sure 'date' is a datetime object\n",
        "\n",
        "        # Ensure you're not comparing dates with prices\n",
        "        if predicted_price > actual_prices[i-1] and balance > actual_price:\n",
        "            shares_to_buy = balance // actual_price\n",
        "            balance -= shares_to_buy * actual_price\n",
        "            total_shares += shares_to_buy\n",
        "            trade_log.append(f\"Bought {shares_to_buy} shares at {actual_price} on {date}, Balance: {balance}, Shares: {total_shares}\")\n",
        "\n",
        "        elif predicted_price < actual_prices[i-1] and total_shares > 0:\n",
        "            balance += total_shares * actual_price\n",
        "            trade_log.append(f\"Sold {total_shares} shares at {actual_price} on {date}, Balance: {balance}\")\n",
        "            total_shares = 0\n",
        "\n",
        "    # Final balance after selling any remaining shares\n",
        "    if total_shares > 0:\n",
        "        balance += total_shares * actual_prices[-1]\n",
        "        trade_log.append(f\"Final Sale of {total_shares} shares at {actual_prices[-1]} on {dates[-1]}, Final Balance: {balance}\")\n",
        "\n",
        "    profit_loss = balance - initial_balance\n",
        "    return trade_log, profit_loss\n",
        "\n",
        "# Step 7: Run the entire pipeline with trading simulation\n",
        "def run_stock_prediction_with_simulation(ticker, period='5y', seq_length=60):\n",
        "    # Download and preprocess the data\n",
        "    stock_data = download_stock_data(ticker, period)\n",
        "    X, y, scaler = preprocess_data(stock_data, seq_length=seq_length)\n",
        "\n",
        "    # Get the corresponding dates for the test set\n",
        "    dates = stock_data.index[seq_length:]  # Dates aligned with the sequences\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Adjust the corresponding dates for the test set\n",
        "    test_dates = dates[-len(X_test):]\n",
        "\n",
        "    # Create and train the LSTM model\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], 1))\n",
        "    model = train_lstm_model(model, X_train, y_train, epochs=10)\n",
        "\n",
        "    # Make predictions and evaluate the model\n",
        "    predictions, rmse = predict_and_evaluate(model, X_test, y_test, scaler)\n",
        "\n",
        "    # Rescale the actual test prices\n",
        "    actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Simulate trading based on model predictions and include dates\n",
        "    trade_log, profit_loss = simulate_trading(predictions.flatten(), actual_prices.flatten(), test_dates)\n",
        "\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "    print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    for log in trade_log:\n",
        "        print(log)\n",
        "\n",
        "    return stock_data, predictions, trade_log, profit_loss\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_symbol = 'GOOG '  # Example stock ticker (Apple Inc.)\n",
        "    stock_data, predictions, trade_log, profit_loss = run_stock_prediction_with_simulation(ticker_symbol)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC-a9TWa-NsB",
        "outputId": "852f7a8b-cb07-4fa8-c905-44bc9cc6599b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 67ms/step - loss: 0.0800\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0106\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0047\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0033\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0028\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - loss: 0.0029\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - loss: 0.0028\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.0028\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0024\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0025\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step\n",
            "Root Mean Squared Error: 6.427794626143496\n",
            "Final Profit/Loss: 1387.219039916994\n",
            "Bought 75.0 shares at 133.32000732421878 on 2023-12-01 00:00:00+00:00, Balance: 0.999450683591931, Shares: 75.0\n",
            "Sold 75.0 shares at 136.63999938964847 on 2023-12-08 00:00:00+00:00, Balance: 10248.999404907227\n",
            "Bought 76.0 shares at 133.97000122070312 on 2023-12-13 00:00:00+00:00, Balance: 67.27931213378906, Shares: 76.0\n",
            "Sold 76.0 shares at 133.1999969482422 on 2023-12-14 00:00:00+00:00, Balance: 10190.479080200195\n",
            "Bought 76.0 shares at 133.83999633789062 on 2023-12-15 00:00:00+00:00, Balance: 18.639358520507812, Shares: 76.0\n",
            "Sold 76.0 shares at 137.19000244140628 on 2023-12-18 00:00:00+00:00, Balance: 10445.079544067385\n",
            "Bought 76.0 shares at 137.38999938964844 on 2024-01-05 00:00:00+00:00, Balance: 3.4395904541033815, Shares: 76.0\n",
            "Sold 76.0 shares at 142.55999755859375 on 2024-01-09 00:00:00+00:00, Balance: 10837.999404907228\n",
            "Bought 75.0 shares at 142.7100067138672 on 2024-02-01 00:00:00+00:00, Balance: 134.74890136718932, Shares: 75.0\n",
            "Sold 75.0 shares at 147.22000122070312 on 2024-02-08 00:00:00+00:00, Balance: 11176.248992919924\n",
            "Bought 78.0 shares at 141.75999450683594 on 2024-02-16 00:00:00+00:00, Balance: 118.96942138672057, Shares: 78.0\n",
            "Sold 78.0 shares at 145.2899932861328 on 2024-02-23 00:00:00+00:00, Balance: 11451.58889770508\n",
            "Bought 81.0 shares at 140.10000610351562 on 2024-02-27 00:00:00+00:00, Balance: 103.48840332031432, Shares: 81.0\n",
            "Sold 81.0 shares at 139.6199951171875 on 2024-03-12 00:00:00+00:00, Balance: 11412.708007812502\n",
            "Bought 63.0 shares at 179.22000122070312 on 2024-07-18 00:00:00+00:00, Balance: 121.84793090820494, Shares: 63.0\n",
            "Sold 63.0 shares at 183.60000610351562 on 2024-07-23 00:00:00+00:00, Balance: 11688.64831542969\n",
            "Bought 69.0 shares at 169.16000366210938 on 2024-07-25 00:00:00+00:00, Balance: 16.608062744142444, Shares: 69.0\n",
            "Sold 69.0 shares at 162.02999877929688 on 2024-08-14 00:00:00+00:00, Balance: 11196.677978515627\n",
            "Bought 68.0 shares at 163.1699981689453 on 2024-08-15 00:00:00+00:00, Balance: 101.11810302734557, Shares: 68.0\n",
            "Sold 68.0 shares at 164.74000549316406 on 2024-08-16 00:00:00+00:00, Balance: 11303.438476562502\n",
            "Bought 71.0 shares at 157.80999755859375 on 2024-09-04 00:00:00+00:00, Balance: 98.92864990234557, Shares: 71.0\n",
            "Sold 71.0 shares at 158.99000549316406 on 2024-09-16 00:00:00+00:00, Balance: 11387.219039916994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KeOfxyNTYap",
        "outputId": "cb0b6426-d00b-4014-8c9a-a6b0a9be5ad2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytrends.request import TrendReq\n",
        "\n",
        "\n",
        "# Step 1.1: Download Google Trends data with additional error handling\n",
        "def download_trends_data(keyword, start_date, end_date):\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    pytrends.build_payload([keyword], timeframe=f'{start_date} {end_date}')\n",
        "    trends_data = pytrends.interest_over_time()\n",
        "\n",
        "\n",
        "    # Check if trends data is empty\n",
        "    if trends_data.empty:\n",
        "        print(f\"No Google Trends data found for keyword: {keyword}\")\n",
        "        return None\n",
        "\n",
        "    # Fill missing values in trends data, if any\n",
        "    trends_data = trends_data.fillna(0)  # You can change the filling strategy as needed\n",
        "\n",
        "    return trends_data\n",
        "\n",
        "\n",
        "from pytrends.request import TrendReq\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Step 1.1: Download Google Trends data with additional error handling and sentiment analysis\n",
        "def download_trends_data(keyword, start_date, end_date):\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    pytrends.build_payload([keyword], timeframe=f'{start_date} {end_date}')\n",
        "    trends_data = pytrends.interest_over_time()\n",
        "\n",
        "    # Check if trends data is empty\n",
        "    if trends_data.empty:\n",
        "        print(f\"No Google Trends data found for keyword: {keyword}\")\n",
        "        return None\n",
        "\n",
        "    # Fill missing values in trends data, if any\n",
        "    trends_data = trends_data.fillna(0)\n",
        "\n",
        "    # Calculate sentiment based on trend score fluctuations\n",
        "    trends_data['change'] = trends_data[keyword].diff()  # Daily change in interest score\n",
        "    trends_data['sentiment'] = trends_data['change'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "\n",
        "    return trends_data\n",
        "\n",
        "# Step 2: Preprocess the stock and trends data together\n",
        "def preprocess_data_with_trends(stock_data, trends_data, feature_col='Close', seq_length=60):\n",
        "    if trends_data is None:\n",
        "        raise ValueError(\"No trends data available, cannot proceed with preprocessing.\")\n",
        "\n",
        "    trends_data.index = trends_data.index.tz_localize(None)\n",
        "    stock_data.index = stock_data.index.tz_localize(None)\n",
        "\n",
        "    # Reindex trends data to match stock data dates\n",
        "    trends_data = trends_data.reindex(stock_data.index, method='ffill').fillna(0)\n",
        "\n",
        "    # Normalize stock prices and trends data separately\n",
        "    stock_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    trend_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    scaled_stock_data = stock_scaler.fit_transform(stock_data[[feature_col]])\n",
        "    scaled_trends_data = trend_scaler.fit_transform(trends_data[['sentiment']])\n",
        "\n",
        "    # Combine the scaled stock prices and trends data\n",
        "    combined_data = np.hstack((scaled_stock_data, scaled_trends_data))\n",
        "\n",
        "    # Create sequences for LSTM input\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(combined_data)):\n",
        "        X.append(combined_data[i-seq_length:i])\n",
        "        y.append(scaled_stock_data[i, 0])  # Stock price (first column)\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    return X, y, stock_scaler\n",
        "\n",
        "# Step 7: Run the entire pipeline with trading simulation, including trends data\n",
        "def run_stock_prediction_with_simulation_and_trends(ticker, keyword, period='5y', seq_length=60):\n",
        "    # Download stock data and Google trends data\n",
        "    stock_data = download_stock_data(ticker, period)\n",
        "    start_date = stock_data.index[0].strftime('%Y-%m-%d')\n",
        "    end_date = stock_data.index[-1].strftime('%Y-%m-%d')\n",
        "\n",
        "    trends_data = download_trends_data(keyword, start_date, end_date)\n",
        "    if trends_data is None:\n",
        "        raise ValueError(\"Google Trends data not available. Exiting...\")\n",
        "\n",
        "    # Preprocess the data (both stock and trends)\n",
        "    X, y, scaler = preprocess_data_with_trends(stock_data, trends_data, seq_length=seq_length)\n",
        "\n",
        "    # Get the corresponding dates for the test set\n",
        "    dates = stock_data.index[seq_length:]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Adjust the corresponding dates for the test set\n",
        "    test_dates = dates[-len(X_test):]\n",
        "\n",
        "    # Create and train the LSTM model\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    model = train_lstm_model(model, X_train, y_train, epochs=10)\n",
        "\n",
        "    # Make predictions and evaluate the model\n",
        "    predictions, rmse = predict_and_evaluate(model, X_test, y_test, scaler)\n",
        "\n",
        "    # Rescale the actual test prices\n",
        "    actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Simulate trading based on model predictions and include dates\n",
        "    trade_log, profit_loss = simulate_trading(predictions.flatten(), actual_prices.flatten(), test_dates)\n",
        "\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "    print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    for log in trade_log:\n",
        "        print(log)\n",
        "\n",
        "    return stock_data, predictions, trade_log, profit_loss\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_symbol = 'GOOG'  # Google stock ticker for Yahoo Finance\n",
        "    keyword = 'Google'  # Keyword for Google Trends to reflect broader market sentiment\n",
        "    stock_data, predictions, trade_log, profit_loss = run_stock_prediction_with_simulation_and_trends(ticker_symbol, keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "xdnlQ0s_TW7V",
        "outputId": "dff5f41a-55e5-42a9-ba49-258ad17f7086"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scaled_trends_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-5d150b11b516>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mticker_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GOOG'\u001b[0m  \u001b[0;31m# Google stock ticker for Yahoo Finance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mkeyword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Google'\u001b[0m  \u001b[0;31m# Keyword for Google Trends to reflect broader market sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mstock_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofit_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_stock_prediction_with_simulation_and_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker_symbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-5d150b11b516>\u001b[0m in \u001b[0;36mrun_stock_prediction_with_simulation_and_trends\u001b[0;34m(ticker, keyword, period, seq_length)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Preprocess the data (both stock and trends)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data_with_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrends_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Get the corresponding dates for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-5d150b11b516>\u001b[0m in \u001b[0;36mpreprocess_data_with_trends\u001b[0;34m(stock_data, trends_data, feature_col, seq_length)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Combine the scaled stock prices and trends data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mcombined_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_stock_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_trends_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Create sequences for LSTM input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scaled_trends_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1.1: Download Google Trends data with additional error handling\n",
        "def download_trends_data(keyword, start_date, end_date):\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    pytrends.build_payload([keyword], timeframe=f'{start_date} {end_date}')\n",
        "    trends_data = pytrends.interest_over_time()\n",
        "\n",
        "    # Check if trends data is empty\n",
        "    if trends_data.empty:\n",
        "        print(f\"No Google Trends data found for keyword: {keyword}\")\n",
        "        return None\n",
        "\n",
        "    # Fill missing values in trends data, if any\n",
        "    trends_data = trends_data.fillna(0)\n",
        "\n",
        "    return trends_data\n",
        "\n",
        "# Step 2: Preprocess the stock and trends data together\n",
        "def preprocess_data_with_trends(stock_data, trends_data, feature_col='Close', seq_length=60):\n",
        "    if trends_data is None:\n",
        "        raise ValueError(\"No trends data available, cannot proceed with preprocessing.\")\n",
        "\n",
        "    trends_data.index = trends_data.index.tz_localize(None)\n",
        "    stock_data.index = stock_data.index.tz_localize(None)\n",
        "\n",
        "    # Reindex trends data to match stock data dates\n",
        "    trends_data = trends_data.reindex(stock_data.index, method='ffill').fillna(0)\n",
        "\n",
        "    # Normalize stock prices and trends data separately\n",
        "    stock_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    trend_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    scaled_stock_data = stock_scaler.fit_transform(stock_data[[feature_col]])\n",
        "    scaled_trends_data = trend_scaler.fit_transform(trends_data[[keyword]])\n",
        "\n",
        "    # Combine the scaled stock prices and trends data\n",
        "    combined_data = np.hstack((scaled_stock_data, scaled_trends_data))\n",
        "\n",
        "    # Create sequences for LSTM input\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(combined_data)):\n",
        "        X.append(combined_data[i-seq_length:i])\n",
        "        y.append(scaled_stock_data[i, 0])  # Stock price (first column)\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    return X, y, stock_scaler\n",
        "\n",
        "# Step 7: Run the entire pipeline with trading simulation, including trends data\n",
        "def run_stock_prediction_with_simulation_and_trends(ticker, keyword, period='5y', seq_length=60):\n",
        "    # Download stock data and Google trends data\n",
        "    stock_data = download_stock_data(ticker, period)\n",
        "    start_date = stock_data.index[0].strftime('%Y-%m-%d')\n",
        "    end_date = stock_data.index[-1].strftime('%Y-%m-%d')\n",
        "\n",
        "    trends_data = download_trends_data(keyword, start_date, end_date)\n",
        "    if trends_data is None:\n",
        "        raise ValueError(\"Google Trends data not available. Exiting...\")\n",
        "\n",
        "    # Preprocess the data (both stock and trends)\n",
        "    X, y, scaler = preprocess_data_with_trends(stock_data, trends_data, seq_length=seq_length)\n",
        "\n",
        "    # Get the corresponding dates for the test set\n",
        "    dates = stock_data.index[seq_length:]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Adjust the corresponding dates for the test set\n",
        "    test_dates = dates[-len(X_test):]\n",
        "\n",
        "    # Create and train the LSTM model\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    model = train_lstm_model(model, X_train, y_train, epochs=10)\n",
        "\n",
        "    # Make predictions and evaluate the model\n",
        "    predictions, rmse = predict_and_evaluate(model, X_test, y_test, scaler)\n",
        "\n",
        "    # Rescale the actual test prices\n",
        "    actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Simulate trading based on model predictions and include dates\n",
        "    trade_log, profit_loss = simulate_trading(predictions.flatten(), actual_prices.flatten(), test_dates)\n",
        "\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "    print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    for log in trade_log:\n",
        "        print(log)\n",
        "\n",
        "    return stock_data, predictions, trade_log, profit_loss\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_symbol = 'GOOG'  # Google stock ticker for Yahoo Finance\n",
        "    keyword = 'Google'  # Keyword for Google Trends to reflect broader market sentiment\n",
        "    stock_data, predictions, trade_log, profit_loss = run_stock_prediction_with_simulation_and_trends(ticker_symbol, keyword)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ2QbaqBd8UQ",
        "outputId": "8a5eb31d-3ac2-46a8-a952-4a392b154a50"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 66ms/step - loss: 0.0854\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0113\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0040\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0036\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0029\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0029\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.0025\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 0.0028\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 0.0027\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 0.0024\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n",
            "Root Mean Squared Error: 5.509690686948932\n",
            "Final Profit/Loss: 285.00842285156614\n",
            "Bought 72.0 shares at 138.6199951171875 on 2023-11-28 00:00:00, Balance: 19.3603515625, Shares: 72.0\n",
            "Sold 72.0 shares at 136.63999938964847 on 2023-12-08 00:00:00, Balance: 9857.44030761719\n",
            "Bought 73.0 shares at 134.6999969482422 on 2023-12-11 00:00:00, Balance: 24.34053039550963, Shares: 73.0\n",
            "Sold 73.0 shares at 138.10000610351562 on 2023-12-19 00:00:00, Balance: 10105.64097595215\n",
            "Bought 71.0 shares at 140.92999267578122 on 2023-12-29 00:00:00, Balance: 99.61149597168333, Shares: 71.0\n",
            "Sold 71.0 shares at 147.97000122070312 on 2024-01-19 00:00:00, Balance: 10605.481582641605\n",
            "Bought 74.0 shares at 142.7100067138672 on 2024-02-01 00:00:00, Balance: 44.941085815433325, Shares: 74.0\n",
            "Sold 74.0 shares at 148.72999572753903 on 2024-02-12 00:00:00, Balance: 11050.960769653322\n",
            "Bought 75.0 shares at 147.13999938964844 on 2024-02-14 00:00:00, Balance: 15.460815429689319, Shares: 75.0\n",
            "Sold 75.0 shares at 139.6199951171875 on 2024-03-12 00:00:00, Balance: 10486.960449218752\n",
            "Bought 68.0 shares at 153.94000244140625 on 2024-04-05 00:00:00, Balance: 19.04028320312682, Shares: 68.0\n",
            "Sold 68.0 shares at 156.13999938964844 on 2024-04-08 00:00:00, Balance: 10636.56024169922\n",
            "Bought 68.0 shares at 156.0 on 2024-04-16 00:00:00, Balance: 28.56024169922057, Shares: 68.0\n",
            "Sold 68.0 shares at 159.9199981689453 on 2024-04-23 00:00:00, Balance: 10903.120117187502\n",
            "Bought 62.0 shares at 173.69000244140625 on 2024-04-26 00:00:00, Balance: 134.33996582031432, Shares: 62.0\n",
            "Sold 62.0 shares at 167.89999389648438 on 2024-04-29 00:00:00, Balance: 10544.139587402346\n",
            "Bought 60.0 shares at 173.9600067138672 on 2024-05-31 00:00:00, Balance: 106.53918457031432, Shares: 60.0\n",
            "Sold 60.0 shares at 178.35000610351562 on 2024-06-06 00:00:00, Balance: 10807.539550781252\n",
            "Bought 61.0 shares at 176.6300048828125 on 2024-06-10 00:00:00, Balance: 33.10925292968932, Shares: 61.0\n",
            "Sold 61.0 shares at 179.55999755859375 on 2024-06-12 00:00:00, Balance: 10986.269104003908\n",
            "Bought 61.0 shares at 178.3699951171875 on 2024-06-14 00:00:00, Balance: 105.69940185547057, Shares: 61.0\n",
            "Sold 61.0 shares at 178.77999877929688 on 2024-06-17 00:00:00, Balance: 11011.27932739258\n",
            "Bought 61.0 shares at 177.7100067138672 on 2024-06-20 00:00:00, Balance: 170.9689178466815, Shares: 61.0\n",
            "Sold 61.0 shares at 180.7899932861328 on 2024-06-24 00:00:00, Balance: 11199.158508300783\n",
            "Bought 59.0 shares at 188.19000244140625 on 2024-07-15 00:00:00, Balance: 95.94836425781432, Shares: 59.0\n",
            "Sold 59.0 shares at 185.5 on 2024-07-16 00:00:00, Balance: 11040.448364257814\n",
            "Bought 60.0 shares at 182.6199951171875 on 2024-07-17 00:00:00, Balance: 83.24865722656432, Shares: 60.0\n",
            "Sold 60.0 shares at 168.96000671386722 on 2024-08-20 00:00:00, Balance: 10220.849060058597\n",
            "Bought 61.0 shares at 167.42999267578125 on 2024-08-23 00:00:00, Balance: 7.619506835941138, Shares: 61.0\n",
            "Sold 61.0 shares at 167.92999267578125 on 2024-08-26 00:00:00, Balance: 10251.349060058597\n",
            "Bought 62.0 shares at 164.5 on 2024-08-28 00:00:00, Balance: 52.34906005859739, Shares: 62.0\n",
            "Sold 62.0 shares at 160.80999755859375 on 2024-09-18 00:00:00, Balance: 10022.56890869141\n",
            "Bought 60.0 shares at 165.6999969482422 on 2024-10-08 00:00:00, Balance: 80.56909179687864, Shares: 60.0\n",
            "Sold 60.0 shares at 163.05999755859375 on 2024-10-09 00:00:00, Balance: 9864.168945312504\n",
            "Bought 60.0 shares at 163.17999267578125 on 2024-10-10 00:00:00, Balance: 73.36938476562864, Shares: 60.0\n",
            "Sold 60.0 shares at 166.89999389648438 on 2024-10-15 00:00:00, Balance: 10087.369018554691\n",
            "Bought 61.0 shares at 165.0500030517578 on 2024-10-18 00:00:00, Balance: 19.318832397464575, Shares: 61.0\n",
            "Sold 61.0 shares at 164.47999572753906 on 2024-10-23 00:00:00, Balance: 10052.598571777347\n",
            "Bought 61.0 shares at 164.52999877929688 on 2024-10-24 00:00:00, Balance: 16.268646240238013, Shares: 61.0\n",
            "Sold 61.0 shares at 168.33999633789062 on 2024-10-28 00:00:00, Balance: 10285.008422851566\n"
          ]
        }
      ]
    }
  ]
}