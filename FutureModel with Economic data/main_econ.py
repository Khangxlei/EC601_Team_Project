# -*- coding: utf-8 -*-
"""main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17gI4wt6P3diFw8_M_dSedXwNwRCCUfnn
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import yfinance as yf
import numpy as np
import pandas as pd
import requests
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from typing import List, Dict, Optional
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define valid periods
valid_periods = ["6mo", "1y", "2y", "5y", "10y", "ytd", "max"]

# FRED API configuration
FRED_API_KEY = "028dd256cbdb1a33b5f5873d225df899"

# Define models
class Trade(BaseModel):
    date: str
    action: str
    shares: int
    price: float
    balance: float

class PredictionItem(BaseModel):
    date: str
    actual: Optional[float] = None
    predicted: float

class PredictionRequest(BaseModel):
    ticker: str
    period: str
    initial_balance: float
    future_days: int = 0

class PredictionResponse(BaseModel):
    initial_balance: float
    final_balance_basic: float
    final_balance_enhanced: float
    profit_loss_basic: Optional[float] = None
    profit_loss_enhanced: Optional[float] = None
    rmse_basic: float
    rmse_enhanced: float
    trade_log_basic: List[Trade]
    trade_log_enhanced: List[Trade]
    predictions_basic: List[PredictionItem]
    predictions_enhanced: List[PredictionItem]

# Utility functions
def fetch_fred_data(series_id, start_date, end_date):
    url = "https://api.stlouisfed.org/fred/series/observations"
    params = {
        "series_id": series_id,
        "api_key": FRED_API_KEY,
        "file_type": "json",
        "observation_start": start_date,
        "observation_end": end_date
    }
    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        df = pd.DataFrame(data["observations"])
        df["date"] = pd.to_datetime(df["date"])
        df["value"] = pd.to_numeric(df["value"], errors='coerce')
        return df.set_index("date")["value"]
    return None

def get_economic_indicators(start_date, end_date):
    indicators = {
        "CPIAUCSL": "inflation",
        "FEDFUNDS": "interest_rate",
        "UNRATE": "unemployment",
        "A191RL1Q225SBEA": "gdp_growth"
    }

    economic_data = pd.DataFrame()
    for series_id, name in indicators.items():
        data = fetch_fred_data(series_id, start_date, end_date)
        if data is not None:
            economic_data[name] = data

    return economic_data.ffill().fillna(0)

def download_stock_data(ticker, period="5y"):
    if period not in valid_periods:
        raise HTTPException(status_code=400, detail=f"Invalid period. Must be one of: {', '.join(valid_periods)}")
    stock_data = yf.download(ticker, period=period)
    return stock_data

def preprocess_basic_data(stock_data, feature_col="Close", seq_length=60):
    try:
        seq_length = int(float(seq_length))
    except (ValueError, TypeError):
        raise HTTPException(status_code=400, detail=f"Invalid sequence length: {seq_length}")

    stock_scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_stock = stock_scaler.fit_transform(stock_data[[feature_col]])

    X, y = [], []
    for i in range(seq_length, len(scaled_stock)):
        X.append(scaled_stock[i-seq_length:i])
        y.append(scaled_stock[i, 0])

    X, y = np.array(X), np.array(y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))
    return X, y, stock_scaler

def preprocess_enhanced_data(stock_data, economic_data, feature_col="Close", seq_length=60):
    try:
        seq_length = int(float(seq_length))
    except (ValueError, TypeError):
        raise HTTPException(status_code=400, detail=f"Invalid sequence length: {seq_length}")

    stock_data.index = stock_data.index.tz_localize(None)
    economic_data.index = economic_data.index.tz_localize(None)

    economic_data = economic_data.reindex(stock_data.index).ffill().fillna(0)

    stock_scaler = MinMaxScaler(feature_range=(0, 1))
    economic_scaler = MinMaxScaler(feature_range=(0, 1))

    scaled_stock = stock_scaler.fit_transform(stock_data[[feature_col]])
    scaled_economic = economic_scaler.fit_transform(economic_data)

    combined_data = np.hstack((scaled_stock, scaled_economic))

    X, y = [], []
    for i in range(seq_length, len(combined_data)):
        X.append(combined_data[i-seq_length:i])
        y.append(scaled_stock[i, 0])

    X, y = np.array(X), np.array(y)
    return X, y, stock_scaler

def create_lstm_model(input_shape):
    model = Sequential([
        LSTM(50, return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        LSTM(25, return_sequences=False),
        Dropout(0.2),
        Dense(10),
        Dense(1)
    ])
    model.compile(optimizer="adam", loss="mse")
    return model

def train_lstm_model(model, X_train, y_train, epochs=10, batch_size=32):
    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)
    return model

def simulate_trading(predictions, actual_prices, dates, initial_balance=10000):
    balance = initial_balance
    shares = 0
    trade_log = []

    for i in range(1, len(predictions)):
        if np.isnan(predictions[i]) or np.isnan(actual_prices[i]):
            continue

        predicted_change = predictions[i] - predictions[i-1]
        current_price = actual_prices[i]

        if predicted_change > 0 and balance >= current_price:
            shares_to_buy = balance // current_price
            balance -= shares_to_buy * current_price
            shares += shares_to_buy
            trade_log.append(Trade(
                date=str(dates[i]),
                action="Bought",
                shares=int(shares_to_buy),
                price=float(current_price),
                balance=float(balance)
            ))

        elif predicted_change < 0 and shares > 0:
            balance += shares * current_price
            trade_log.append(Trade(
                date=str(dates[i]),
                action="Sold",
                shares=int(shares),
                price=float(current_price),
                balance=float(balance)
            ))
            shares = 0

    if shares > 0:
        balance += shares * actual_prices[-1]
        trade_log.append(Trade(
            date=str(dates[-1]),
            action="Sold",
            shares=int(shares),
            price=float(actual_prices[-1]),
            balance=float(balance)
        ))

    profit_loss = balance - initial_balance
    return trade_log, profit_loss

@app.post("/api/predict", response_model=PredictionResponse)
async def predict_stock(request: PredictionRequest):
    try:
        if request.period not in valid_periods:
            raise HTTPException(status_code=400,
                              detail=f"Invalid period. Must be one of: {', '.join(valid_periods)}")

        if request.future_days < 0 or request.future_days > 365:
            raise HTTPException(status_code=400,
                              detail="Future days must be between 0 and 365")

        # Download data
        stock_data = download_stock_data(request.ticker, request.period)
        start_date = stock_data.index[0].strftime('%Y-%m-%d')
        end_date = stock_data.index[-1].strftime('%Y-%m-%d')
        economic_data = get_economic_indicators(start_date, end_date)

        # Prepare both datasets
        X_basic, y_basic, scaler_basic = preprocess_basic_data(stock_data)
        X_enhanced, y_enhanced, scaler_enhanced = preprocess_enhanced_data(stock_data, economic_data)

        # Split data
        split_idx = int(len(X_basic) * 0.8)

        # Basic model data
        X_train_basic = X_basic[:split_idx]
        X_test_basic = X_basic[split_idx:]
        y_train_basic = y_basic[:split_idx]
        y_test_basic = y_basic[split_idx:]

        # Enhanced model data
        X_train_enhanced = X_enhanced[:split_idx]
        X_test_enhanced = X_enhanced[split_idx:]
        y_train_enhanced = y_enhanced[:split_idx]
        y_test_enhanced = y_enhanced[split_idx:]

        # Train both models
        model_basic = create_lstm_model((X_train_basic.shape[1], 1))
        model_basic = train_lstm_model(model_basic, X_train_basic, y_train_basic)

        model_enhanced = create_lstm_model((X_train_enhanced.shape[1], X_train_enhanced.shape[2]))
        model_enhanced = train_lstm_model(model_enhanced, X_train_enhanced, y_train_enhanced)

        # Make predictions
        predictions_basic = model_basic.predict(X_test_basic)
        predictions_basic = scaler_basic.inverse_transform(predictions_basic)
        actual_prices_basic = scaler_basic.inverse_transform(y_test_basic.reshape(-1, 1))

        predictions_enhanced = model_enhanced.predict(X_test_enhanced)
        predictions_enhanced = scaler_enhanced.inverse_transform(predictions_enhanced)
        actual_prices_enhanced = scaler_enhanced.inverse_transform(y_test_enhanced.reshape(-1, 1))

        # Handle NaN values
        predictions_basic = np.nan_to_num(predictions_basic, nan=0.0)
        predictions_enhanced = np.nan_to_num(predictions_enhanced, nan=0.0)
        actual_prices_basic = np.nan_to_num(actual_prices_basic, nan=0.0)
        actual_prices_enhanced = np.nan_to_num(actual_prices_enhanced, nan=0.0)

        # Calculate RMSE
        rmse_basic = float(np.sqrt(np.mean((predictions_basic - actual_prices_basic) ** 2)))
        rmse_enhanced = float(np.sqrt(np.mean((predictions_enhanced - actual_prices_enhanced) ** 2)))

        # Generate prediction lists
        test_dates = stock_data.index[split_idx + 60:].strftime("%Y-%m-%d").tolist()

        predictions_list_basic = []
        predictions_list_enhanced = []

        for i in range(len(test_dates)):
            pred_basic = float(predictions_basic[i][0])
            pred_enhanced = float(predictions_enhanced[i][0])
            actual = float(actual_prices_basic[i][0])

            if np.isfinite(pred_basic) and np.isfinite(actual):
                predictions_list_basic.append(PredictionItem(
                    date=test_dates[i],
                    actual=actual,
                    predicted=pred_basic
                ))

            if np.isfinite(pred_enhanced) and np.isfinite(actual):
                predictions_list_enhanced.append(PredictionItem(
                    date=test_dates[i],
                    actual=actual,
                    predicted=pred_enhanced
                ))

        # Simulate trading
        trade_log_basic, profit_loss_basic = simulate_trading(
            predictions_basic.flatten(),
            actual_prices_basic.flatten(),
            test_dates,
            request.initial_balance
        )

        trade_log_enhanced, profit_loss_enhanced = simulate_trading(
            predictions_enhanced.flatten(),
            actual_prices_enhanced.flatten(),
            test_dates,
            request.initial_balance
        )

        return PredictionResponse(
            initial_balance=request.initial_balance,
            final_balance_basic=trade_log_basic[-1].balance if trade_log_basic else request.initial_balance,
            final_balance_enhanced=trade_log_enhanced[-1].balance if trade_log_enhanced else request.initial_balance,
            profit_loss_basic=profit_loss_basic,
            profit_loss_enhanced=profit_loss_enhanced,
            rmse_basic=rmse_basic,
            rmse_enhanced=rmse_enhanced,
            trade_log_basic=trade_log_basic,
            trade_log_enhanced=trade_log_enhanced,
            predictions_basic=predictions_list_basic,
            predictions_enhanced=predictions_list_enhanced
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

