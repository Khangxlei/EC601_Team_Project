{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Step 1: Download stock data from Yahoo Finance\n",
        "def download_stock_data(ticker, period='5y'):\n",
        "    stock_data = yf.download(ticker, period=period)\n",
        "    return stock_data\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "def preprocess_data(data, feature_col='Close', seq_length=60):\n",
        "    # Use 'Close' prices to predict trends\n",
        "    data = data[[feature_col]]\n",
        "\n",
        "    # Normalize the data using MinMaxScaler\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "    # Create sequences of data points for LSTM input\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(scaled_data)):\n",
        "        X.append(scaled_data[i-seq_length:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape the data to be compatible with LSTM (samples, timesteps, features)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    return X, y, scaler\n",
        "\n",
        "# Step 3: Build the LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(units=1))  # Predicting a single output value (next price)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Step 4: Train the LSTM model\n",
        "def train_lstm_model(model, X_train, y_train, epochs=50, batch_size=64):\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "    return model\n",
        "\n",
        "# Step 5: Make predictions and evaluate\n",
        "def predict_and_evaluate(model, X_test, y_test, stock_scaler):\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Inverse transform only the stock price column\n",
        "    predictions_stock_price = stock_scaler.inverse_transform(predictions)\n",
        "    y_test_stock_price = stock_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Calculate the Root Mean Squared Error (RMSE) on the stock prices\n",
        "    rmse = np.sqrt(np.mean((predictions_stock_price - y_test_stock_price) ** 2))\n",
        "    return predictions_stock_price, rmse\n",
        "\n",
        "# Step 6: Trading simulation logic\n",
        "def simulate_trading(predictions, actual_prices, dates, initial_balance=10000, shares=0):\n",
        "    balance = initial_balance\n",
        "    total_shares = shares\n",
        "    trade_log = []\n",
        "\n",
        "    for i in range(1, len(predictions)):\n",
        "        predicted_price = predictions[i]\n",
        "        actual_price = actual_prices[i]\n",
        "        date = dates[i]  # Make sure 'date' is a datetime object\n",
        "\n",
        "        # Ensure you're not comparing dates with prices\n",
        "        if predicted_price > actual_prices[i-1] and balance > actual_price:\n",
        "            shares_to_buy = balance // actual_price\n",
        "            balance -= shares_to_buy * actual_price\n",
        "            total_shares += shares_to_buy\n",
        "            trade_log.append(f\"Bought {shares_to_buy} shares at {actual_prices[i][0]:.2f} on {dates[i]}\")\n",
        "\n",
        "        elif predicted_price < actual_prices[i-1] and total_shares > 0:\n",
        "            balance += total_shares * actual_price\n",
        "            trade_log.append(f\"Sold {total_shares} shares at {actual_price[0]:.2f} on {date}, Balance: {balance:.2f}\")\n",
        "            total_shares = 0\n",
        "\n",
        "    # Final balance after selling any remaining shares\n",
        "    if total_shares > 0:\n",
        "        balance += total_shares * actual_prices[-1]\n",
        "        trade_log.append(f\"Final Sale of {total_shares} shares at {actual_prices[-1][0]:.2f} on {dates[-1]}, Final Balance: {balance:.2f}\")\n",
        "\n",
        "\n",
        "    profit_loss = balance - initial_balance\n",
        "    return trade_log, profit_loss\n"
      ],
      "metadata": {
        "id": "kBSjrkNl9wxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Trading simulation logic\n",
        "def simulate_trading(predictions, actual_prices, dates, initial_balance=10000, shares=0):\n",
        "    balance = initial_balance\n",
        "    total_shares = shares\n",
        "    trade_log = []\n",
        "\n",
        "    for i in range(1, len(predictions)):\n",
        "        predicted_price = predictions[i]\n",
        "        actual_price = actual_prices[i]\n",
        "        date = dates[i]  # Make sure 'date' is a datetime object\n",
        "\n",
        "        # Ensure you're not comparing dates with prices\n",
        "        if predicted_price > actual_prices[i-1] and balance > actual_price:\n",
        "            shares_to_buy = balance // actual_price\n",
        "            balance -= shares_to_buy * actual_price\n",
        "            total_shares += shares_to_buy\n",
        "            trade_log.append(f\"Bought {shares_to_buy} shares at {actual_price} on {date}, Balance: {balance}, Shares: {total_shares}\")\n",
        "\n",
        "        elif predicted_price < actual_prices[i-1] and total_shares > 0:\n",
        "            balance += total_shares * actual_price\n",
        "            trade_log.append(f\"Sold {total_shares} shares at {actual_price} on {date}, Balance: {balance}\")\n",
        "            total_shares = 0\n",
        "\n",
        "    # Final balance after selling any remaining shares\n",
        "    if total_shares > 0:\n",
        "        balance += total_shares * actual_prices[-1]\n",
        "        trade_log.append(f\"Final Sale of {total_shares} shares at {actual_prices[-1]} on {dates[-1]}, Final Balance: {balance}\")\n",
        "\n",
        "    profit_loss = balance - initial_balance\n",
        "    return trade_log, profit_loss\n",
        "\n",
        "# Step 7: Run the entire pipeline with trading simulation\n",
        "def run_stock_prediction_with_simulation(ticker, period='5y', seq_length=60):\n",
        "    # Download and preprocess the data\n",
        "    stock_data = download_stock_data(ticker, period)\n",
        "    X, y, scaler = preprocess_data(stock_data, seq_length=seq_length)\n",
        "\n",
        "    # Get the corresponding dates for the test set\n",
        "    dates = stock_data.index[seq_length:]  # Dates aligned with the sequences\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Adjust the corresponding dates for the test set\n",
        "    test_dates = dates[-len(X_test):]\n",
        "\n",
        "    # Create and train the LSTM model\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], 1))\n",
        "    model = train_lstm_model(model, X_train, y_train, epochs=10)\n",
        "\n",
        "    # Make predictions and evaluate the model\n",
        "    predictions, rmse = predict_and_evaluate(model, X_test, y_test, scaler)\n",
        "\n",
        "    # Rescale the actual test prices\n",
        "    actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # Simulate trading based on model predictions and include dates\n",
        "    trade_log, profit_loss = simulate_trading(predictions.flatten(), actual_prices.flatten(), test_dates)\n",
        "\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "    print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    for log in trade_log:\n",
        "        print(log)\n",
        "\n",
        "    return stock_data, predictions, trade_log, profit_loss\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_symbol = 'GOOG '  # Example stock ticker (Apple Inc.)\n",
        "    stock_data, predictions, trade_log, profit_loss = run_stock_prediction_with_simulation(ticker_symbol)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC-a9TWa-NsB",
        "outputId": "1528ac56-1744-43f5-c28a-ea7fcd570436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 0.0729\n",
            "Epoch 2/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0096\n",
            "Epoch 3/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - loss: 0.0046\n",
            "Epoch 4/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0037\n",
            "Epoch 5/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0030\n",
            "Epoch 6/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0027\n",
            "Epoch 7/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0030\n",
            "Epoch 8/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - loss: 0.0029\n",
            "Epoch 9/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.0025\n",
            "Epoch 10/10\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.0025\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n",
            "Root Mean Squared Error: 7.104833450695313\n",
            "Final Profit/Loss: 1329.2990417480505\n",
            "Bought 75.0 shares at 133.32000732421878 on 2023-12-01 00:00:00+00:00, Balance: 0.999450683591931, Shares: 75.0\n",
            "Sold 75.0 shares at 136.63999938964847 on 2023-12-08 00:00:00+00:00, Balance: 10248.999404907227\n",
            "Bought 71.0 shares at 142.7100067138672 on 2024-02-01 00:00:00+00:00, Balance: 116.58892822265625, Shares: 71.0\n",
            "Sold 71.0 shares at 146.67999267578128 on 2024-02-07 00:00:00+00:00, Balance: 10530.868408203127\n",
            "Bought 74.0 shares at 141.75999450683594 on 2024-02-16 00:00:00+00:00, Balance: 40.628814697267444, Shares: 74.0\n",
            "Sold 74.0 shares at 145.32000732421875 on 2024-02-22 00:00:00+00:00, Balance: 10794.309356689455\n",
            "Bought 77.0 shares at 140.10000610351562 on 2024-02-27 00:00:00+00:00, Balance: 6.608886718751819, Shares: 77.0\n",
            "Sold 77.0 shares at 139.6199951171875 on 2024-03-12 00:00:00+00:00, Balance: 10757.34851074219\n",
            "Bought 59.0 shares at 179.38999938964844 on 2024-07-19 00:00:00+00:00, Balance: 173.3385467529315, Shares: 59.0\n",
            "Sold 59.0 shares at 183.60000610351562 on 2024-07-23 00:00:00+00:00, Balance: 11005.738906860353\n",
            "Bought 65.0 shares at 169.16000366210938 on 2024-07-25 00:00:00+00:00, Balance: 10.338668823244006, Shares: 65.0\n",
            "Sold 65.0 shares at 172.4499969482422 on 2024-08-01 00:00:00+00:00, Balance: 11219.588470458986\n",
            "Bought 69.0 shares at 160.6399993896484 on 2024-08-05 00:00:00+00:00, Balance: 135.42851257324583, Shares: 69.0\n",
            "Sold 69.0 shares at 163.9499969482422 on 2024-08-12 00:00:00+00:00, Balance: 11447.978302001957\n",
            "Bought 68.0 shares at 165.92999267578125 on 2024-08-13 00:00:00+00:00, Balance: 164.73880004883176, Shares: 68.0\n",
            "Sold 68.0 shares at 162.02999877929688 on 2024-08-14 00:00:00+00:00, Balance: 11182.77871704102\n",
            "Bought 68.0 shares at 163.1699981689453 on 2024-08-15 00:00:00+00:00, Balance: 87.21884155273801, Shares: 68.0\n",
            "Sold 68.0 shares at 164.74000549316406 on 2024-08-16 00:00:00+00:00, Balance: 11289.539215087894\n",
            "Bought 71.0 shares at 157.80999755859375 on 2024-09-04 00:00:00+00:00, Balance: 85.02938842773801, Shares: 71.0\n",
            "Sold 71.0 shares at 158.3699951171875 on 2024-09-13 00:00:00+00:00, Balance: 11329.29904174805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KeOfxyNTYap",
        "outputId": "a3a26597-9b11-40f8-94da-f14593d483c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytrends\n",
            "Successfully installed pytrends-4.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytrends.request import TrendReq\n",
        "\n",
        "\n",
        "# Step 1.1: Download Google Trends data with additional error handling\n",
        "def download_trends_data(keyword, start_date, end_date):\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    pytrends.build_payload([keyword], timeframe=f'{start_date} {end_date}')\n",
        "    trends_data = pytrends.interest_over_time()\n",
        "\n",
        "\n",
        "    # Check if trends data is empty\n",
        "    if trends_data.empty:\n",
        "        print(f\"No Google Trends data found for keyword: {keyword}\")\n",
        "        return None\n",
        "\n",
        "    # Fill missing values in trends data, if any\n",
        "    trends_data = trends_data.fillna(0)  # You can change the filling strategy as needed\n",
        "\n",
        "    return trends_data\n",
        "\n",
        "\n",
        "from pytrends.request import TrendReq\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Step 1.1: Download Google Trends data with additional error handling and sentiment analysis\n",
        "def download_trends_data(keyword, start_date, end_date):\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    pytrends.build_payload([keyword], timeframe=f'{start_date} {end_date}')\n",
        "    trends_data = pytrends.interest_over_time()\n",
        "\n",
        "    # Check if trends data is empty\n",
        "    if trends_data.empty:\n",
        "        print(f\"No Google Trends data found for keyword: {keyword}\")\n",
        "        return None\n",
        "\n",
        "    # Fill missing values in trends data, if any\n",
        "    trends_data = trends_data.fillna(0)\n",
        "\n",
        "    # Calculate sentiment based on trend score fluctuations\n",
        "    trends_data['change'] = trends_data[keyword].diff()  # Daily change in interest score\n",
        "    trends_data['sentiment'] = trends_data['change'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "\n",
        "    return trends_data\n",
        "\n",
        "# Step 2: Preprocess the stock and trends data together\n",
        "def preprocess_data_with_trends(stock_data, trends_data, feature_col='Close', seq_length=60):\n",
        "    if trends_data is None:\n",
        "        raise ValueError(\"No trends data available, cannot proceed with preprocessing.\")\n",
        "\n",
        "    trends_data.index = trends_data.index.tz_localize(None)\n",
        "    stock_data.index = stock_data.index.tz_localize(None)\n",
        "\n",
        "    # Reindex trends data to match stock data dates\n",
        "    trends_data = trends_data.reindex(stock_data.index, method='ffill').fillna(0)\n",
        "\n",
        "    # Normalize stock prices and trends data separately\n",
        "    stock_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    trend_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    scaled_stock_data = stock_scaler.fit_transform(stock_data[[feature_col]])\n",
        "    scaled_trends_data = trend_scaler.fit_transform(trends_data[['sentiment']])\n",
        "\n",
        "    # Combine the scaled stock prices and trends data\n",
        "    combined_data = np.hstack((scaled_stock_data, scaled_trends_data))\n",
        "\n",
        "    # Create sequences for LSTM input\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(combined_data)):\n",
        "        X.append(combined_data[i-seq_length:i])\n",
        "        y.append(scaled_stock_data[i, 0])  # Stock price (first column)\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    return X, y, stock_scaler\n",
        "\n",
        "# Example function to simulate trading with more realistic features\n",
        "def simulate_trading_with_strategy(predictions, actual_prices, dates, initial_capital=10000, fee_rate=0.001, buy_threshold=0.01, sell_threshold=-0.01):\n",
        "    capital = initial_capital\n",
        "    holdings = 0  # Number of shares held\n",
        "    trade_log = []\n",
        "    actual_prices = actual_prices.flatten()\n",
        "\n",
        "    for i in range(1, len(predictions)):\n",
        "        predicted_change = predictions[i] - predictions[i - 1]\n",
        "        actual_change = actual_prices[i] - actual_prices[i - 1]\n",
        "\n",
        "        # Buy condition: only if the predicted increase is above the buy threshold and we have capital\n",
        "        if predicted_change > buy_threshold and capital > actual_prices[i]:\n",
        "            # Buy as many shares as possible within the available capital\n",
        "            shares_to_buy = capital // actual_prices[i]\n",
        "            capital -= shares_to_buy * actual_prices[i] * (1 + fee_rate)  # Deduct cost with fee\n",
        "            holdings += shares_to_buy\n",
        "            trade_log.append(f\"Bought {shares_to_buy} shares at {actual_prices[i]:.2f} on {dates[i]}\")\n",
        "\n",
        "        # Sell condition: only if the predicted decrease is below the sell threshold and we have holdings\n",
        "        elif predicted_change < sell_threshold and holdings > 0:\n",
        "            capital += holdings * actual_prices[i] * (1 - fee_rate)  # Sell all holdings, account for fee\n",
        "            trade_log.append(f\"Sold {holdings} shares at {actual_prices[i]:.2f} on {dates[i]}\")\n",
        "            holdings = 0  # Reset holdings after selling\n",
        "\n",
        "    # Calculate final portfolio value\n",
        "    final_value = capital + holdings * actual_prices[-1]\n",
        "    profit_loss = final_value - initial_capital\n",
        "\n",
        "    return trade_log, profit_loss, final_value\n",
        "\n",
        "# # Example usage of the function\n",
        "# trade_log, profit_loss, final_value = simulate_trading_with_strategy(predictions, actual_prices, test_dates)\n",
        "# print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "# print(f\"Final Portfolio Value: {final_value}\")\n",
        "# for log in trade_log:\n",
        "#     print(log)\n",
        "\n",
        "\n",
        "# Step 7: Run the entire pipeline with trading simulation, including trends data\n",
        "def run_stock_prediction_with_simulation_and_trends(ticker, keyword, period='5y', seq_length=60):\n",
        "    # Download stock data and Google trends data\n",
        "    stock_data = download_stock_data(ticker, period)\n",
        "    start_date = stock_data.index[0].strftime('%Y-%m-%d')\n",
        "    end_date = stock_data.index[-1].strftime('%Y-%m-%d')\n",
        "\n",
        "    trends_data = download_trends_data(keyword, start_date, end_date)\n",
        "    if trends_data is None:\n",
        "        raise ValueError(\"Google Trends data not available. Exiting...\")\n",
        "\n",
        "    # Preprocess the data (both stock and trends)\n",
        "    X, y, scaler = preprocess_data_with_trends(stock_data, trends_data, seq_length=seq_length)\n",
        "\n",
        "    # Get the corresponding dates for the test set\n",
        "    dates = stock_data.index[seq_length:]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Adjust the corresponding dates for the test set\n",
        "    test_dates = dates[-len(X_test):]\n",
        "\n",
        "    # Create and train the LSTM model\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    model = train_lstm_model(model, X_train, y_train, epochs=50)\n",
        "\n",
        "    # Make predictions and evaluate the model\n",
        "    predictions, rmse = predict_and_evaluate(model, X_test, y_test, scaler)\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "    # Rescale the actual test prices\n",
        "    actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    # # Simulate trading based on model predictions and include dates\n",
        "    # trade_log, profit_loss = simulate_trading(predictions.flatten(), actual_prices.flatten(), test_dates)\n",
        "\n",
        "    # print(f\"Root Mean Squared Error: {rmse}\")\n",
        "    # print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    # for log in trade_log:\n",
        "    #     print(log)\n",
        "\n",
        "    trade_log, profit_loss, final_value = simulate_trading_with_strategy(predictions, actual_prices, test_dates)\n",
        "    print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    print(f\"Final Portfolio Value: {final_value}\")\n",
        "    for log in trade_log:\n",
        "        print(log)\n",
        "\n",
        "    return stock_data, predictions, trade_log, profit_loss\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_symbol = 'GOOG'  # Google stock ticker for Yahoo Finance\n",
        "    keyword = 'Google'  # Keyword for Google Trends to reflect broader market sentiment\n",
        "    stock_data, predictions, trade_log, profit_loss = run_stock_prediction_with_simulation_and_trends(ticker_symbol, keyword)\n",
        "    # 1584.8979492187518\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdnlQ0s_TW7V",
        "outputId": "e698a0a2-c56c-44a1-b1fd-d435f8b150d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step - loss: 0.0856\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - loss: 0.0107\n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 0.0053\n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - loss: 0.0041\n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0033\n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0030\n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0026\n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0023\n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0023\n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0027\n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - loss: 0.0027\n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - loss: 0.0021\n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 0.0022\n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.0025\n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0024\n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0022\n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0022\n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0022\n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0019\n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0020\n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0022\n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - loss: 0.0021\n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - loss: 0.0019\n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 0.0020\n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0019\n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0017\n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0017\n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0017\n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - loss: 0.0016\n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0016\n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - loss: 0.0016\n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 0.0014\n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0018\n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0014\n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0013\n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0015\n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0015\n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0016\n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0014\n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0014\n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - loss: 0.0013\n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - loss: 0.0014\n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - loss: 0.0014\n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 0.0015\n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0014\n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0011\n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0013\n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0013\n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0013\n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0012\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step\n",
            "Root Mean Squared Error: 5.383997173329514\n",
            "Final Profit/Loss: 1935.1982524414252\n",
            "Final Portfolio Value: 11935.198252441425\n",
            "Bought 72.0 shares at 138.62 on 2023-11-21 00:00:00\n",
            "Sold 72.0 shares at 133.92 on 2023-11-30 00:00:00\n",
            "Bought 71.0 shares at 134.70 on 2023-12-11 00:00:00\n",
            "Sold 71.0 shares at 140.36 on 2024-01-03 00:00:00\n",
            "Bought 69.0 shares at 143.67 on 2024-01-11 00:00:00\n",
            "Sold 69.0 shares at 142.71 on 2024-02-01 00:00:00\n",
            "Bought 66.0 shares at 148.73 on 2024-02-12 00:00:00\n",
            "Sold 66.0 shares at 142.20 on 2024-02-20 00:00:00\n",
            "Bought 67.0 shares at 139.62 on 2024-03-12 00:00:00\n",
            "Sold 67.0 shares at 156.14 on 2024-04-08 00:00:00\n",
            "Bought 66.0 shares at 158.14 on 2024-04-09 00:00:00\n",
            "Sold 66.0 shares at 156.88 on 2024-04-17 00:00:00\n",
            "Bought 64.0 shares at 161.10 on 2024-04-24 00:00:00\n",
            "Sold 64.0 shares at 175.13 on 2024-06-04 00:00:00\n",
            "Bought 64.0 shares at 175.95 on 2024-06-07 00:00:00\n",
            "Sold 64.0 shares at 178.19 on 2024-06-11 00:00:00\n",
            "Bought 64.0 shares at 176.45 on 2024-06-18 00:00:00\n",
            "Sold 64.0 shares at 188.19 on 2024-07-15 00:00:00\n",
            "Bought 69.0 shares at 174.37 on 2024-07-24 00:00:00\n",
            "Sold 69.0 shares at 169.16 on 2024-07-25 00:00:00\n",
            "Bought 70.0 shares at 168.40 on 2024-08-19 00:00:00\n",
            "Sold 70.0 shares at 163.40 on 2024-08-29 00:00:00\n",
            "Bought 71.0 shares at 160.81 on 2024-09-18 00:00:00\n",
            "Sold 71.0 shares at 168.56 on 2024-10-04 00:00:00\n",
            "Bought 72.0 shares at 164.39 on 2024-10-07 00:00:00\n",
            "Sold 72.0 shares at 165.70 on 2024-10-08 00:00:00\n",
            "Bought 72.0 shares at 166.90 on 2024-10-15 00:00:00\n",
            "Sold 72.0 shares at 164.53 on 2024-10-24 00:00:00\n",
            "Bought 69.0 shares at 171.14 on 2024-10-29 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download the VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "# Step 1.1: Fetch Google Trends data as a sentiment proxy\n",
        "def fetch_trends_data_for_keyword(keyword, start_date, end_date):\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    pytrends.build_payload([keyword], timeframe=f'{start_date} {end_date}')\n",
        "    trends_data = pytrends.interest_over_time()\n",
        "\n",
        "    # Check if trends data is empty\n",
        "    if trends_data.empty:\n",
        "        print(f\"No Google Trends data found for keyword: {keyword}\")\n",
        "        return None\n",
        "\n",
        "    # Process trends data as a sentiment proxy\n",
        "    trends_data = trends_data[keyword]  # Select the interest data for the keyword\n",
        "    trends_data = trends_data.fillna(0)  # Fill missing values, if any\n",
        "\n",
        "    return trends_data\n",
        "\n",
        "# Step 1.2: Updated function to calculate sentiment scores based on trends data (using trend level as sentiment proxy)\n",
        "def download_sentiment_data(keyword, start_date, end_date):\n",
        "    trends_data = fetch_trends_data_for_keyword(keyword, start_date, end_date)\n",
        "    if trends_data is None:\n",
        "        return None\n",
        "\n",
        "    # Calculate daily average as sentiment proxy (optional adjustment)\n",
        "    sentiment_series = trends_data.resample('D').mean().fillna(0)  # Fill missing dates with zero interest\n",
        "    return sentiment_series\n",
        "\n",
        "\n",
        "# Step 2: Preprocess stock and trends data together, adding sentiment data\n",
        "def preprocess_data_with_trends_and_sentiment(stock_data, trends_data, sentiment_data, feature_col='Close', seq_length=60):\n",
        "    if trends_data is None or sentiment_data is None:\n",
        "        raise ValueError(\"Trends or sentiment data not available, cannot proceed with preprocessing.\")\n",
        "\n",
        "    trends_data.index = trends_data.index.tz_localize(None)\n",
        "    stock_data.index = stock_data.index.tz_localize(None)\n",
        "    sentiment_data.index = sentiment_data.index.tz_localize(None)\n",
        "\n",
        "    # Reindex trends and sentiment data to match stock data dates\n",
        "    trends_data = trends_data.reindex(stock_data.index, method='ffill').fillna(0)\n",
        "    sentiment_data = sentiment_data.reindex(stock_data.index, method='ffill').fillna(0)\n",
        "\n",
        "    # Normalize stock prices, trends, and sentiment data separately\n",
        "    stock_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    trend_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    sentiment_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    scaled_stock_data = stock_scaler.fit_transform(stock_data[[feature_col]])\n",
        "    scaled_trends_data = trend_scaler.fit_transform(trends_data[[keyword]])\n",
        "    scaled_sentiment_data = sentiment_scaler.fit_transform(sentiment_data.values.reshape(-1, 1))\n",
        "\n",
        "    # Combine scaled stock prices, trends, and sentiment data\n",
        "    combined_data = np.hstack((scaled_stock_data, scaled_trends_data, scaled_sentiment_data))\n",
        "\n",
        "    # Create sequences for LSTM input\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(combined_data)):\n",
        "        X.append(combined_data[i-seq_length:i])\n",
        "        y.append(scaled_stock_data[i, 0])  # Stock price (first column)\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    return X, y, stock_scaler\n",
        "\n",
        "# Example usage in the pipeline\n",
        "def run_stock_prediction_with_sentiment(ticker, keyword, period='5y', seq_length=60):\n",
        "    stock_data = download_stock_data(ticker, period)\n",
        "    start_date = stock_data.index[0].strftime('%Y-%m-%d')\n",
        "    end_date = stock_data.index[-1].strftime('%Y-%m-%d')\n",
        "\n",
        "    trends_data = download_trends_data(keyword, start_date, end_date)\n",
        "    sentiment_data = download_sentiment_data(keyword, start_date, end_date)\n",
        "\n",
        "    X, y, scaler = preprocess_data_with_trends_and_sentiment(stock_data, trends_data, sentiment_data, seq_length=seq_length)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    test_dates = stock_data.index[seq_length + len(X_train):]\n",
        "\n",
        "\n",
        "    model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
        "    model = train_lstm_model(model, X_train, y_train, epochs=50)\n",
        "\n",
        "    predictions, rmse = predict_and_evaluate(model, X_test, y_test, scaler)\n",
        "    print(f\"Root Mean Squared Error: {rmse}\")\n",
        "\n",
        "        # Rescale the actual test prices for trading simulation\n",
        "    actual_prices = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Run the trading simulation based on predictions\n",
        "    trade_log, profit_loss, final_value = simulate_trading_with_strategy(predictions.flatten(), actual_prices, test_dates)\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Final Profit/Loss: {profit_loss}\")\n",
        "    print(f\"Final Portfolio Value: {final_value}\")\n",
        "    for log in trade_log:\n",
        "        print(log)\n",
        "\n",
        "    return stock_data, predictions, trade_log, profit_loss\n",
        "\n",
        "\n",
        "# Run example\n",
        "if __name__ == \"__main__\":\n",
        "    ticker_symbol = 'GOOG'\n",
        "    keyword = 'Google'\n",
        "    run_stock_prediction_with_sentiment(ticker_symbol, keyword)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ2QbaqBd8UQ",
        "outputId": "5b8c4c84-3555-4f0b-add2-19f400bde7b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 111ms/step - loss: 0.1156\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 118ms/step - loss: 0.0103\n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - loss: 0.0045\n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0034\n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.0032\n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0027\n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0029\n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0025\n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0025\n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0024\n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - loss: 0.0022\n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 103ms/step - loss: 0.0020\n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 0.0022\n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0021\n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0020\n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0020\n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0019\n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0021\n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0018\n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 0.0019\n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 114ms/step - loss: 0.0019\n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - loss: 0.0018\n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0017\n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0018\n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0018\n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0019\n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0016\n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0017\n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0015\n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0017\n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - loss: 0.0016\n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.0015\n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.0014\n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0016\n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0014\n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0016\n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0015\n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0014\n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0014\n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 115ms/step - loss: 0.0013\n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 0.0012\n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - loss: 0.0013\n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0014\n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0013\n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 0.0013\n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - loss: 0.0014\n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - loss: 0.0012\n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - loss: 0.0015\n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - loss: 0.0014\n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 112ms/step - loss: 0.0012\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
            "Root Mean Squared Error: 4.783371406470234\n",
            "Final Profit/Loss: 1906.213030288709\n",
            "Final Portfolio Value: 11906.213030288709\n",
            "Bought 72.0 shares at 138.62 on 2023-11-21 00:00:00\n",
            "Sold 72.0 shares at 133.92 on 2023-11-30 00:00:00\n",
            "Bought 72.0 shares at 133.64 on 2023-12-12 00:00:00\n",
            "Sold 72.0 shares at 137.39 on 2024-01-05 00:00:00\n",
            "Bought 68.0 shares at 144.24 on 2024-01-12 00:00:00\n",
            "Sold 68.0 shares at 142.71 on 2024-02-01 00:00:00\n",
            "Bought 65.0 shares at 148.73 on 2024-02-12 00:00:00\n",
            "Sold 65.0 shares at 141.76 on 2024-02-16 00:00:00\n",
            "Bought 66.0 shares at 139.62 on 2024-03-12 00:00:00\n",
            "Sold 66.0 shares at 157.46 on 2024-04-18 00:00:00\n",
            "Bought 64.0 shares at 161.10 on 2024-04-24 00:00:00\n",
            "Sold 64.0 shares at 174.42 on 2024-06-03 00:00:00\n",
            "Bought 63.0 shares at 178.35 on 2024-06-06 00:00:00\n",
            "Sold 63.0 shares at 185.50 on 2024-07-16 00:00:00\n",
            "Bought 69.0 shares at 168.96 on 2024-08-20 00:00:00\n",
            "Sold 69.0 shares at 163.40 on 2024-08-29 00:00:00\n",
            "Bought 70.0 shares at 160.81 on 2024-09-18 00:00:00\n",
            "Sold 70.0 shares at 165.70 on 2024-10-08 00:00:00\n",
            "Bought 69.0 shares at 166.74 on 2024-10-16 00:00:00\n",
            "Sold 69.0 shares at 166.99 on 2024-10-25 00:00:00\n",
            "Bought 69.0 shares at 168.34 on 2024-10-28 00:00:00\n"
          ]
        }
      ]
    }
  ]
}